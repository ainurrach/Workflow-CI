# -*- coding: utf-8 -*-
"""Modelling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DYMX3L8UF1JVJeR0SWy-KGBTVEV9Qiv_
"""


import mlflow
import mlflow.sklearn
from pyngrok import ngrok
from google.colab import drive
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import os

# 2. Mount Google Drive
drive.mount('/content/drive')

# Buat folder khusus MLflow di Drive jika belum ada
save_path = "/content/drive/MyDrive/mlflow_experiments"
if not os.path.exists(save_path):
    os.makedirs(save_path)

# 3. Konfigurasi MLflow Database (SQLite di Drive agar Cepat & Permanen)
db_path = f"sqlite:///{save_path}/mlflow.db"
mlflow.set_tracking_uri(db_path)
mlflow.set_experiment("Model_Klasifikasi_Colab")

import pandas as pd
import mlflow
import mlflow.sklearn

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report # Import classification_report

import os

# 4. Aktifkan Autolog (Mencatat parameter, metrik, dan model secara otomatis)
mlflow.sklearn.autolog()

def train_model():
    # Load Dataset
    try:
        # Sesuaikan path jika file berada di folder tertentu
        df = pd.read_csv('bank_marketing_preprocessing.csv')
    except:
        print("File tidak ditemukan! Pastikan file sudah di-upload.")
        return

    # Handle NaN values by dropping rows with any missing values
    # Alternatively, you could use imputation (e.g., df.fillna(df.mean()) or SimpleImputer)
    df = df.dropna()
    if df.empty:
        print("DataFrame is empty after dropping NaN values. Please check your data.")
        return

    # Asumsi: Kolom target bernama 'y' atau 'deposit'. Silakan sesuaikan.
    # Jika Anda belum tahu nama kolom targetnya, gunakan df.columns
    target_col = 'y' if 'y' in df.columns else 'deposit' # Assume 'deposit' if 'y' is not found
    if target_col not in df.columns:
        print(f"Target column '{target_col}' not found in the DataFrame. Available columns: {df.columns.tolist()}")
        return

    X = df.drop(columns=[target_col])
    y = df[target_col]

    # Convert categorical features to numerical using one-hot encoding
    X = pd.get_dummies(X, drop_first=True)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    with mlflow.start_run(run_name="Random_Forest_Optimization"):
        # Training Model
        model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
        model.fit(X_train, y_train)

        # Evaluasi
        predictions = model.predict(X_test)
        report = classification_report(y_test, predictions)
        print("Model Training Selesai. Hasil:")
        print(report)

        # Log tambahan jika diperlukan (opsional karena autolog sudah mencatat banyak hal)
        mlflow.log_param("dataset_rows", len(df))

# Jalankan proses training
train_model()

import subprocess
import time
from pyngrok import ngrok

# 1. Konfigurasi Token
NGROK_AUTH_TOKEN = "371cOqgCjs8FUXagS1MbrxgtpD8_7nM5CCQMjfKdKMGwcFfbP"
ngrok.set_auth_token(NGROK_AUTH_TOKEN)

# 2. Bersihkan Sesi Sebelumnya
print("Cleaning up sessions...")
!fuser -k 5000/tcp
ngrok.kill()

# 3. Jalankan MLflow Server
# Gunakan sqlite agar lebih stabil untuk UI
db_path = "sqlite:///mlflow.db"
print("Starting MLflow Server...")
subprocess.Popen(["mlflow", "ui", "--backend-store-uri", db_path, "--port", "5000", "--host", "0.0.0.0"])

# 4. Tunggu Server Inisialisasi
time.sleep(10)

# 5. Buka Tunnel dengan host_header yang benar
try:
    # Menggunakan parameter host_header langsung di dalam connect
    public_url = ngrok.connect(5000, host_header="localhost:5000")
    print(f"\n‚úÖ BERHASIL!")
    print(f"üîó Buka MLflow UI di sini: {public_url}")
except Exception as e:
    print(f"‚ùå Gagal: {e}")
